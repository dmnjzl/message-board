Thanks for the nice intro article on neural networks! It's a really good write up with good figures and tables and illustrates the basic concepts very well. However, the following are some opinions I've picked up about neural networks from various colleagues, professors, and others so far, and I feel pretty strongly about this topic. 



1) Neural networks are not built after how the brain works. If anything, this is likely just a good PR (public relations) naming move, which machine learning has been very good with in general to make their ideas sound super cool (ex. deep learning, random forests, etc...), unlike statisticians (who have uncool sounding methods like regression and tests).

2) Neural networks are a very broad class of models. In a sense, you can structure your neural network so that it performs the same as linear regression or logistic regression, and it's possible that you could even design your neural network so that it performs the same as a random forest.

3) Neural networks I believe were initially developed for image processing. 

4) Neural networks can be a powerful tool, and more work should be done to figure out how neural networks can be best applied in different settings outside of machine learning (ex. one of the famous biostatisticians at Dana Farber Cancer center here has some post docs trying to tailor neural networks to cancer genomics). However, what you'll find most often in practice in industry and novice applications of neural networks is people don't give much thought to how they build their networks (they'll just thrown in a bunch of hidden layers and decide on random activation functions. This quote from the github articles summarizes this perfectly. 

"There are many types of activation functions—linear, sigmoid, hyperbolic tangent, even step-wise. To be honest, I don’t know why one function is better than another."

5) There are big issues with interpretability in neural networks. A lot of times, the first layer of a neural network it's more likely to be interpretable as the first layer nodes are a direct function of your inputs. However as you add more and more layers, you have more and more of a black box algorithm. Some people will try to make up meaning for different layers, but if you randomly decide on how you build your neural network and don't know why you're making the choices you are, you're pretty much making stuff up. 

6) If you're mainly interested in prediction issues, you might be more OK with this kind of approach, and if you're a business that just wants a random prediction and don't care about the why, you might be OK with this too. For example, if Netflix wants to predict your ratings for movies you haven't seen, they don't care why you would like a movie, they just care if they can guess whether you like it or not. But if you are more interested in scientific or clinical research and care about the "why", these "black box" approaches might not be what you're looking for.
 

